{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95340c2e",
   "metadata": {},
   "source": [
    "Updates: Pyspark is now installed on my local computer. It turned out to be difficult to load the data from my S3 bucket because dataframes in Pyspark appear to load lazily (if I create a temporary directory to store the files \"downloaded\" from the S3 bucket, then try to access the dataframe outside of the \"with\" loop in which the temporary directory exists, it throws a \"File Not Found\" error.)\n",
    "\n",
    "So for now, I'm creating a proof-of-concept model with the 1 million record sample I generated using Pandas earlier which I saved to a file in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc43846",
   "metadata": {},
   "source": [
    "Import Pyspark, start a Spark Session, load the transactions data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc706e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdbda53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/29 19:49:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('H&M Recommendation System').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f31de0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "transactions = spark.read.csv('data/transactions_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2ac9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+----------------+\n",
      "|     _c0|     t_dat|         customer_id|article_id|               price|sales_channel_id|\n",
      "+--------+----------+--------------------+----------+--------------------+----------------+\n",
      "| 6819188|2019-02-28|cf4254f236a7d3f4d...| 639192002| 0.06777966101694917|               1|\n",
      "| 1937255|2018-10-30|9da5d0fc26ed57853...| 652361003|0.008457627118644067|               2|\n",
      "|30574525|2020-08-21|52d409edafe2ab3c9...| 877273001| 0.03388135593220338|               1|\n",
      "| 6990198|2019-03-05|7022f60068ed9d8cb...| 714543001| 0.01254237288135593|               1|\n",
      "|18501631|2019-11-01|4d4f9a5031d13fdd4...| 765853019| 0.06777966101694917|               2|\n",
      "+--------+----------+--------------------+----------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 19:50:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , t_dat, customer_id, article_id, price, sales_channel_id\n",
      " Schema: _c0, t_dat, customer_id, article_id, price, sales_channel_id\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/user/Documents/Flatiron/phase_4/notes/rec_system__h-m/data/transactions_sample.csv\n"
     ]
    }
   ],
   "source": [
    "transactions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a7260",
   "metadata": {},
   "source": [
    "All datatypes are **string**.\n",
    "\n",
    "| Least Recent Transaction | Most Recent Transaction |\n",
    "| --- | --- |\n",
    "| 9/20/2018 | 9/22/2020 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7dddd5",
   "metadata": {},
   "source": [
    "Filter for the most recent transactions, and determine customer-article purchase counts (\"historical_ratings\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88899b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want the most recent year of transactions\n",
    "recent_transactions = transactions.filter(transactions.t_dat > '2019-9-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c958bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(avg(count)=1.4835677120257107)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_counts = recent_transactions.groupBy('customer_id').count()\n",
    "purchase_counts.agg(F.avg('count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69b3d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(stddev_samp(count)=0.9896841504041872)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_counts.agg(F.stddev('count')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ecd9c",
   "metadata": {},
   "source": [
    "On average, each customer in our sample made 1.5 purchases. (Standard deviation of nearly 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44a9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_ratings = recent_transactions.groupBy(['customer_id', 'article_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4743d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|         customer_id|article_id|count|\n",
      "+--------------------+----------+-----+\n",
      "|adde61206265d6e1a...| 859400006|    1|\n",
      "|577a62fedef3ba595...| 828268001|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "historical_ratings.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975ea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(max(count)=5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_ratings.agg({'count': 'max'}).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd97ffc",
   "metadata": {},
   "source": [
    "The most number of times a customer has purchased an article in this dataset is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4ddaa",
   "metadata": {},
   "source": [
    "Train-test-split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43b46e",
   "metadata": {},
   "source": [
    "Perform a stratified train-test-split (try to keep 80% of each article_id in the training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa34f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fractions = historical_ratings.select('article_id').distinct()\\\n",
    ".withColumn(\"fraction\", F.lit(0.8)).rdd.collectAsMap()\n",
    "\n",
    "train = historical_ratings.sampleBy(col='article_id',\n",
    "                                    fractions=fractions,\n",
    "                                    seed=312)\n",
    "\n",
    "test = historical_ratings.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e85366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|         customer_id|article_id|count|\n",
      "+--------------------+----------+-----+\n",
      "|adde61206265d6e1a...| 859400006|    1|\n",
      "|577a62fedef3ba595...| 828268001|    1|\n",
      "|4f0b41a56d8fc0cc8...| 877961021|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5ea9e",
   "metadata": {},
   "source": [
    "Transform features / clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b5726",
   "metadata": {},
   "source": [
    "Change all relevant columns to integer type before fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e806bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('article_id', train.article_id.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af9eb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "si = StringIndexer(inputCol='customer_id',\n",
    "                   outputCol='customer_index',\n",
    "                   handleInvalid='keep')\n",
    "\n",
    "new_train = si.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146fdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.withColumn('customer_index', new_train.customer_index.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55691c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.withColumn('count', new_train['count'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2659dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 19:51:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+\n",
      "|         customer_id|article_id|count|customer_index|\n",
      "+--------------------+----------+-----+--------------+\n",
      "|adde61206265d6e1a...| 859400006|    1|         39570|\n",
      "|577a62fedef3ba595...| 828268001|    1|          3396|\n",
      "+--------------------+----------+-----+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f731277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:17:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:17:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:56 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:18:59 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:02 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:08 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:09 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:18 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:27 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:31 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:35 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:19:45 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALSModel: uid=ALS_c14c9207315a, rank=10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "%timeit\n",
    "\n",
    "als = ALS(rank=10,\n",
    "          userCol='customer_index',\n",
    "          itemCol='article_id',\n",
    "          ratingCol='count',\n",
    "          implicitPrefs=True,\n",
    "          nonnegative=True,\n",
    "          coldStartStrategy='drop',\n",
    "          seed=312\n",
    "         )\n",
    "\n",
    "model = als.fit(new_train)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c19086c",
   "metadata": {},
   "source": [
    "---\n",
    "Here are my efforts to use the mllib version of ALS rather than the ml version, so that I can calculate MAP@3 efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f663cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuples = new_train.withColumnRenamed('count', 'rating').drop('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1f401f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 19:51:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+\n",
      "|article_id|rating|customer_index|\n",
      "+----------+------+--------------+\n",
      "| 859400006|     1|         39570|\n",
      "| 828268001|     1|          3396|\n",
      "+----------+------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_tuples.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65db9b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 19:17:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 19:17:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 19:17:21 ERROR Executor: Exception in task 0.0 in stage 68.0 (TID 1195)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py\", line 234, in <lambda>\n",
      "    ratings = ratings.map(lambda x: Rating(*x))\n",
      "TypeError: __new__() missing 2 required positional arguments: 'product' and 'rating'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/29 19:17:21 WARN TaskSetManager: Lost task 0.0 in stage 68.0 (TID 1195, rebeccas-mbp.lan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py\", line 234, in <lambda>\n",
      "    ratings = ratings.map(lambda x: Rating(*x))\n",
      "TypeError: __new__() missing 2 required positional arguments: 'product' and 'rating'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "23/03/29 19:17:21 ERROR TaskSetManager: Task 0 in stage 68.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o429.trainImplicitALSModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 1195, rebeccas-mbp.lan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py\", line 234, in <lambda>\n    ratings = ratings.map(lambda x: Rating(*x))\nTypeError: __new__() missing 2 required positional arguments: 'product' and 'rating'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1531)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1531)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:241)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainImplicitALSModel(PythonMLLibAPI.scala:514)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py\", line 234, in <lambda>\n    ratings = ratings.map(lambda x: Rating(*x))\nTypeError: __new__() missing 2 required positional arguments: 'product' and 'rating'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecommendation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALS \u001b[38;5;28;01mas\u001b[39;00m ALS2\n\u001b[0;32m----> 3\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mALS2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainImplicit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ratings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py:314\u001b[0m, in \u001b[0;36mALS.trainImplicit\u001b[0;34m(cls, ratings, rank, iterations, lambda_, blocks, alpha, nonnegative, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.9.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrainImplicit\u001b[39m(\u001b[38;5;28mcls\u001b[39m, ratings, rank, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m    280\u001b[0m                   nonnegative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Train a matrix factorization model given an RDD of 'implicit\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    preferences' of users for a subset of products. The ratings matrix\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m      (default: None)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcallMLlibFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainImplicitALSModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m                          \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatrixFactorizationModel(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/common.py:130\u001b[0m, in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m    129\u001b[0m api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonMLLibAPI(), name)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/common.py:123\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Call Java Function \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:131\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    133\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o429.trainImplicitALSModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 1195, rebeccas-mbp.lan, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py\", line 234, in <lambda>\n    ratings = ratings.map(lambda x: Rating(*x))\nTypeError: __new__() missing 2 required positional arguments: 'product' and 'rating'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n\tat org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1531)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1531)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:241)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainImplicitALSModel(PythonMLLibAPI.scala:514)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/user/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py\", line 234, in <lambda>\n    ratings = ratings.map(lambda x: Rating(*x))\nTypeError: __new__() missing 2 required positional arguments: 'product' and 'rating'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:266)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1423)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS as ALS2\n",
    "\n",
    "model2 = ALS2.trainImplicit(train_ratings.rdd, 10, nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed93a277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:05:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings=[Rating(x.asDict()['customer_index'],\n",
    "                x.asDict()['article_id'],\n",
    "                x.asDict()['rating']) for x in train_tuples.sample(\n",
    "    fraction=0.1, seed=312).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a2d67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_rdd = spark.sparkContext.parallelize(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14078bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:07:55 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/03/29 20:07:55 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS as ALS2\n",
    "\n",
    "model2 = ALS2.trainImplicit(ratings_rdd, 10, nonnegative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df018dba",
   "metadata": {},
   "source": [
    "What we need is a list of the top 3 recommended items for each user, zipped with a \"ground truth\" from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c361ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 10:38:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+\n",
      "|         customer_id|article_id|count|customer_index|\n",
      "+--------------------+----------+-----+--------------+\n",
      "|beb07a60fa9baaa9f...| 859092001|    1|        197933|\n",
      "|f06e69b81c840b5c0...| 727948001|    1|        197933|\n",
      "+--------------------+----------+-----+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e34156bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'asDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustomer_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masDict\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'asDict'"
     ]
    }
   ],
   "source": [
    "new_test.filter(new_test.customer_index == 5).select('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72cb49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 11:02:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(article_id=621381001),\n",
       " Row(article_id=719629024),\n",
       " Row(article_id=864309001),\n",
       " Row(article_id=492897001),\n",
       " Row(article_id=621381012)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test.filter(new_test.customer_index == 5).select('article_id').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "60b186a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1691:================================>                     (39 + 8) / 64]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecommendations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecommendations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "recommendations.filter(recommendations._1 == 1).select('_2').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c3221158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 11:23:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:24:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:25:33 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:26:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:27:08 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:27:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:28:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:29:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:30:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:30:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:31:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:32:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:33:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:33:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:34:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:35:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:36:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:36:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:39:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:39:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:40:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:41:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:42:05 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:42:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:43:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:44:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:45:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:45:50 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:46:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:47:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:48:08 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:48:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:49:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:50:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:51:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:51:57 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:52:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:53:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:54:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:55:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:55:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:56:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:57:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:58:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:58:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 11:59:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:00:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:01:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:02:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:02:50 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:03:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:04:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:05:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:06:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:06:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:07:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:08:28 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:09:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:10:02 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:10:45 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:11:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:12:16 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:13:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:13:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:14:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:15:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:16:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:17:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:17:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:18:44 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:19:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:20:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:21:09 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:21:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:22:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:23:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:24:09 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:24:57 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:25:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:26:28 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:27:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:28:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:28:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:29:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:30:28 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:31:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:32:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:32:50 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:33:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:34:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:35:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:36:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:36:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:37:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 12:38:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:39:23 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:40:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:41:02 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:41:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:42:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:43:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:44:13 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:44:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:45:45 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:46:33 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:47:18 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:48:05 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:48:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:49:44 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:50:27 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:51:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:52:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:53:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:53:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:54:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:55:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 12:56:16 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:12:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:13:13 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:13:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:14:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:15:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:16:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:17:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:18:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:18:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:19:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:20:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:21:23 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:22:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:23:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:23:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:24:33 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:25:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:26:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:26:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:27:31 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:28:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:29:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:29:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:30:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:31:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:32:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:33:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:34:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:34:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:35:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:36:27 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:37:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:38:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:38:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:39:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:40:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:41:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:42:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:43:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:44:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:44:56 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:45:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:46:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:47:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:48:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:48:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:49:35 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:50:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:51:16 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:52:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:52:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:53:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:54:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:55:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:56:13 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:57:02 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:57:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:58:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 13:59:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:00:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:01:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:01:56 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:02:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:03:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:04:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:05:09 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:05:56 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:06:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:07:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:08:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:09:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 14:10:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:11:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:11:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:12:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:13:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:15:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:16:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:16:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:17:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:18:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:19:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:20:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:20:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:21:45 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:22:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:23:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:24:16 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:25:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 14:25:50 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 16:20:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 18:48:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 21:09:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/30 23:54:56 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 02:37:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 05:02:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 07:32:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:54:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:55:35 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:56:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:57:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:58:08 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:58:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 09:59:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:00:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:01:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:02:31 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:03:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:04:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:04:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:12:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:13:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:14:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:15:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:16:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:17:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:17:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:18:44 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:19:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:20:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:21:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:22:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:22:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:23:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:24:28 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:25:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:26:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:26:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:27:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:28:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:29:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:30:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:31:13 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:32:02 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:32:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:33:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:34:35 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:38:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:39:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:40:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:41:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:41:50 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:42:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:43:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:44:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:44:57 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:46:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:47:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:48:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:49:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:50:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:51:05 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:51:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:52:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:53:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:54:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:55:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:55:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:56:44 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:57:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:58:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 10:59:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:00:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:01:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:02:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/31 11:02:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:03:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:04:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:05:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:06:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:07:09 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:08:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:08:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:09:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:10:28 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:11:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:12:04 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:12:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:13:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:14:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:15:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:16:13 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:17:05 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:17:57 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:18:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:19:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:20:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:21:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:22:18 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:23:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:24:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:24:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:25:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:26:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:27:22 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:28:17 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:29:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:29:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:30:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:31:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:32:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:33:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:34:18 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:35:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:35:59 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:36:50 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:37:45 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:38:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:39:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:40:23 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:41:18 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:42:08 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:43:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:43:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:44:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:45:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:46:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:47:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:48:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:48:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:49:45 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:50:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:51:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:52:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:53:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 11:53:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:09:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:25:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:26:31 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:27:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:28:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:29:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:30:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:30:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:31:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:32:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:33:15 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:34:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:35:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:35:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:36:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:37:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:38:27 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 12:55:59 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:05:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:05:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:06:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:07:34 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:08:29 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:09:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:10:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:11:05 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:12:00 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:12:51 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:13:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:14:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:15:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:16:14 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:17:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/31 13:17:57 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:18:49 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:19:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:20:31 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:21:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:22:11 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:23:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:23:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:24:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:25:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:26:24 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:27:12 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:27:58 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:28:54 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:29:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:30:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:31:26 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:32:16 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:33:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:34:03 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:34:59 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:35:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:36:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:37:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:38:37 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:39:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:40:38 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:41:32 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:42:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:43:27 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:44:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:45:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:46:05 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:47:01 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:47:53 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:48:47 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:49:43 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:50:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:51:35 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:52:21 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 13:53:10 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 14:03:42 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/31 14:04:30 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "[Stage 10182:=======>   (144 + 8) / 200][Stage 10183:>            (8 + 0) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m rows1 \u001b[38;5;241m=\u001b[39m recommendations\u001b[38;5;241m.\u001b[39mfilter(recommendations\u001b[38;5;241m.\u001b[39m_1\u001b[38;5;241m==\u001b[39mcustomer_index)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m rankings \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows1]\n\u001b[0;32m----> 7\u001b[0m rows2 \u001b[38;5;241m=\u001b[39m \u001b[43mnew_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustomer_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcustomer_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m rows2]\n\u001b[1;32m     10\u001b[0m ranking_tuples\u001b[38;5;241m.\u001b[39mappend((rankings, ground_truth))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:596\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m>>> df.collect()\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m[Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 596\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ranking_tuples = []\n",
    "\n",
    "for customer_index in [x['_1'] for x in recommendations.select('_1').distinct().collect()]:\n",
    "    rows1 = recommendations.filter(recommendations._1==customer_index).select('_2').collect()[0]['_2']\n",
    "    rankings = [row['product'] for row in rows1]\n",
    "    \n",
    "    rows2 = new_test.filter(new_test.customer_index == customer_index).select('article_id').collect()\n",
    "    ground_truth = [x['article_id'] for x in rows2]\n",
    "    \n",
    "    ranking_tuples.append((rankings, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef539dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_tuples.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb8718a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendations = model2.recommendProductsForUsers(3).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8aa9f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1540:=========================================>            (49 + 8) / 64]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[751471001, 741356002, 372860001]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row['product'] for row in recommendations.filter(recommendations._1==5).select('_2').collect()[0]['_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "de37559d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o442.recommendProducts. Trace:\npy4j.Py4JException: Method recommendProducts([class java.lang.String, class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_test\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommended\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecommendProducts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/recommendation.py:183\u001b[0m, in \u001b[0;36mMatrixFactorizationModel.recommendProducts\u001b[0;34m(self, user, num)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommendProducts\u001b[39m(\u001b[38;5;28mself\u001b[39m, user, num):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    Recommends the top \"num\" number of products for a given user and\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    returns a list of Rating objects sorted by the predicted rating in\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    descending order.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecommendProducts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/common.py:146\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[38;5;241m*\u001b[39ma):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124;03m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/common.py:123\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\" Call Java Function \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:131\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    133\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o442.recommendProducts. Trace:\npy4j.Py4JException: Method recommendProducts([class java.lang.String, class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\n"
     ]
    }
   ],
   "source": [
    "new_test.withColumn('recommended', model2.recommendProducts('customer_index', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.recommendProducts(user, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "abc3b11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.recommendation.MatrixFactorizationModel at 0x7ff6f08d8a30>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67d2ee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.000714322093696623, 1.0), (3.472187358199223e-11, 1.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = ratings_rdd.map(lambda p: (p.user, p.product))\n",
    "predictions = model2.predictAll(testData).map(lambda r: ((r.user, r.product), r.rating))\n",
    "\n",
    "ratingsTuple = ratings_rdd.map(lambda r: ((r.user, r.product), r.rating))\n",
    "scoreAndLabels = predictions.join(ratingsTuple).map(lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3513c2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.000714322093696623, 1.0),\n",
       " (3.472187358199223e-11, 1.0),\n",
       " (5.869481761830604e-07, 1.0),\n",
       " (0.01919505331824229, 1.0),\n",
       " (1.0814860265047868e-06, 1.0)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreAndLabels.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3c3fd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "baseline_model_metrics = RankingMetrics(scoreAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c61d9d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 10:35:48 ERROR Executor: Exception in task 3.0 in stage 1363.0 (TID 8153)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 0.0 in stage 1363.0 (TID 8150)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 5.0 in stage 1363.0 (TID 8155)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 7.0 in stage 1363.0 (TID 8157)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 4.0 in stage 1363.0 (TID 8154)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 6.0 in stage 1363.0 (TID 8156)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 2.0 in stage 1363.0 (TID 8152)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 ERROR Executor: Exception in task 1.0 in stage 1363.0 (TID 8151)\n",
      "java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/30 10:35:48 WARN TaskSetManager: Lost task 2.0 in stage 1363.0 (TID 8152, rebeccas-mbp.lan, executor driver): java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n",
      "\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n",
      "\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n",
      "\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "23/03/30 10:35:48 ERROR TaskSetManager: Task 2 in stage 1363.0 failed 1 times; aborting job\n",
      "\r",
      "[Stage 1363:>                                                      (0 + 1) / 16]\r",
      "23/03/30 10:35:48 WARN TaskSetManager: Lost task 8.0 in stage 1363.0 (TID 8158, rebeccas-mbp.lan, executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1051.meanAveragePrecisionAt.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1363.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1363.0 (TID 8152, rebeccas-mbp.lan, executor driver): java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1076)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$1(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$mean$1(DoubleRDDFunctions.scala:48)\n\tat scala.runtime.java8.JFunction0$mcD$sp.apply(JFunction0$mcD$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:48)\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecisionAt(RankingMetrics.scala:90)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbaseline_model_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeanAveragePrecisionAt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/evaluation.py:453\u001b[0m, in \u001b[0;36mRankingMetrics.meanAveragePrecisionAt\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeanAveragePrecisionAt\u001b[39m(\u001b[38;5;28mself\u001b[39m, k):\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    Returns the mean average precision (MAP) at first k ranking of all the queries.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    If a query has an empty ground truth set, the average precision will be zero and\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    a log warining is generated.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeanAveragePrecisionAt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/common.py:146\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[38;5;241m*\u001b[39ma):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124;03m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/mllib/common.py:123\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\" Call Java Function \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:131\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    133\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1051.meanAveragePrecisionAt.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1363.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1363.0 (TID 8152, rebeccas-mbp.lan, executor driver): java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1076)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$1(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.stats(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$mean$1(DoubleRDDFunctions.scala:48)\n\tat scala.runtime.java8.JFunction0$mcD$sp.apply(JFunction0$mcD$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.mean(DoubleRDDFunctions.scala:48)\n\tat org.apache.spark.mllib.evaluation.RankingMetrics.meanAveragePrecisionAt(RankingMetrics.scala:90)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassCastException: class java.lang.Double cannot be cast to class scala.collection.Seq (java.lang.Double is in module java.base of loader 'bootstrap'; scala.collection.Seq is in unnamed module of loader 'app')\n\tat org.apache.spark.sql.Row.getSeq(Row.scala:317)\n\tat org.apache.spark.sql.Row.getSeq$(Row.scala:317)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.$anonfun$newRankingMetrics$1(PythonMLLibAPI.scala:1065)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$stats$2(DoubleRDDFunctions.scala:43)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "baseline_model_metrics.meanAveragePrecisionAt(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc48192",
   "metadata": {},
   "source": [
    "---\n",
    "Back to the ml.recommendation ALS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbd29194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:20:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:20:46 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:20:48 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 20:21:15 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 20:21:52 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "[Stage 783:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+-------------+\n",
      "|         customer_id|article_id|count|customer_index|   prediction|\n",
      "+--------------------+----------+-----+--------------+-------------+\n",
      "|bee1609ea3ae56cf2...| 126589006|    1|        160676|          0.0|\n",
      "|c8d946756335665f1...| 201219001|    1|          4929| 3.4413097E-7|\n",
      "|7960918e6f907cca6...| 201219001|    1|        120418| 1.5044737E-8|\n",
      "|d827509e61518f012...| 201219001|    1|         14210| 2.4846895E-7|\n",
      "|a6a5de89a30288d32...| 201219001|    1|         38539|1.47105625E-8|\n",
      "+--------------------+----------+-----+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 783:=========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_predictions = model.transform(new_train)\n",
    "train_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c9fe0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_recs = model.recommendForAllUsers(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df9d4abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test = test.withColumn('article_id', test.article_id.cast('int'))\n",
    "\n",
    "new_test = si.fit(train).transform(test)\n",
    "\n",
    "new_test = new_test.withColumn('customer_index', new_test.customer_index.cast('int'))\n",
    "\n",
    "new_test = new_test.withColumn('count', new_test['count'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df24176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:25:19 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+\n",
      "|         customer_id|article_id|count|customer_index|\n",
      "+--------------------+----------+-----+--------------+\n",
      "|beb07a60fa9baaa9f...| 859092001|    1|        197933|\n",
      "+--------------------+----------+-----+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de799a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:27:46 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 20:29:00 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "196724"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_customers = top_3_recs.select('customer_index').distinct().count()\n",
    "num_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "feb21193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 15:41:28 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 15:42:52 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# if you remove the filter, you can get metrics for ALL recommendations =)\n",
    "\n",
    "recommended = [ x['article_id'] for x in top_5_recommendations.filter(\n",
    "    top_5_recommendations.customer_index == 1\n",
    ").collect()[0]['recommendations'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1c262441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 15:40:55 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ground_truth = [x['article_id'] for x in new_test.filter(new_test.customer_index==1).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f9a574b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended:\n",
      "[872105001, 926921002, 689898002, 899364002, 870351001]\n",
      "\n",
      "Ground Truth:\n",
      "[838787005, 859176002]\n"
     ]
    }
   ],
   "source": [
    "print('Recommended:')\n",
    "print(recommended)\n",
    "print('\\nGround Truth:')\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8f81b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sparkContext.parallelize([(recommended, ground_truth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "007af58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "baseline_model_metrics = RankingMetrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "19fa1d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model_metrics.precisionAt(3) # all are zero because I didn't get any correct xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6fa7819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "274708"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.select('customer_index').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bff838b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 16:29:29 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 16:30:44 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[877607001, 852940001, 848175001, 872105001, 868060005]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['article_id'] for x in top_5_recommendations.filter(\"customer_index = 5\").select('recommendations').collect()[0]['recommendations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33807098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'customer_index[article_id]'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['article_id'] for x in top_3_recs.filter('customer_index = 5')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc3d9897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:35:26 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 20:36:36 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(recommendations=[Row(article_id=372860001, rating=0.015275676734745502), Row(article_id=791587001, rating=0.012520085088908672), Row(article_id=448509014, rating=0.010398919694125652)])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_recs.filter('customer_index = 5').select('recommendations').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ccdc0b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 21:12:56 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 21:14:06 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Row(article_id=372860001, rating=0.015275676734745502),\n",
       "  Row(article_id=791587001, rating=0.012520085088908672),\n",
       "  Row(article_id=448509014, rating=0.010398919694125652)]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in top_3_recs.filter('customer_index = 5').select('recommendations').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b5efda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 21:20:15 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 21:21:25 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[372860001]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0][i]['article_id'] for i, x in enumerate(top_3_recs.filter('customer_index=5').select('recommendations').collect())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4e858db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 10:13:27 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/30 10:14:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(recommendations=[Row(article_id=372860001, rating=0.015275676734745502), Row(article_id=791587001, rating=0.012520085088908672), Row(article_id=448509014, rating=0.010398919694125652)])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_recs.filter('customer_index=5').select('recommendations').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce1e07be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 10:15:09 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/30 10:16:20 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(recommendations=[Row(article_id=372860001, rating=0.015275676734745502), Row(article_id=791587001, rating=0.012520085088908672), Row(article_id=448509014, rating=0.010398919694125652)])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_recs.filter('customer_index=5').select('recommendations').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36adb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/30 10:25:29 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/30 10:26:40 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(article_id=372860001, rating=0.015275676734745502),\n",
       " Row(article_id=791587001, rating=0.012520085088908672),\n",
       " Row(article_id=448509014, rating=0.010398919694125652)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_recs.filter('customer_index=5').select('recommendations').collect()[0]['recommendations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d561fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 20:39:58 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 20:41:07 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m customer_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_customers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m): \u001b[38;5;66;03m# don't include \"catch-all\" customer index\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     customer_recs \u001b[38;5;241m=\u001b[39m top_3_recs\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_index = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustomer_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m----> 6\u001b[0m     recommended\u001b[38;5;241m.\u001b[39mappend([x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m customer_recs])\n\u001b[1;32m      7\u001b[0m     recommended\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m      8\u001b[0m         x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m top_3_recs\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_index = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustomer_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m         )\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\\\n\u001b[1;32m     10\u001b[0m                    [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m                   ])\n\u001b[1;32m     12\u001b[0m     ground_truth\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m     13\u001b[0m         x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m new_test\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     14\u001b[0m             new_test\u001b[38;5;241m.\u001b[39mcustomer_index\u001b[38;5;241m==\u001b[39mcustomer_index\n\u001b[1;32m     15\u001b[0m         )\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     16\u001b[0m     ])\n",
      "Cell \u001b[0;32mIn[82], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m customer_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_customers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m): \u001b[38;5;66;03m# don't include \"catch-all\" customer index\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     customer_recs \u001b[38;5;241m=\u001b[39m top_3_recs\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_index = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustomer_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m----> 6\u001b[0m     recommended\u001b[38;5;241m.\u001b[39mappend([\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecommendations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m customer_recs])\n\u001b[1;32m      7\u001b[0m     recommended\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m      8\u001b[0m         x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m top_3_recs\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_index = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustomer_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m         )\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\\\n\u001b[1;32m     10\u001b[0m                    [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m                   ])\n\u001b[1;32m     12\u001b[0m     ground_truth\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m     13\u001b[0m         x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m new_test\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     14\u001b[0m             new_test\u001b[38;5;241m.\u001b[39mcustomer_index\u001b[38;5;241m==\u001b[39mcustomer_index\n\u001b[1;32m     15\u001b[0m         )\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     16\u001b[0m     ])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "recommended = []\n",
    "ground_truth = []\n",
    "\n",
    "for customer_index in range(num_customers - 2): # don't include \"catch-all\" customer index\n",
    "    customer_recs = top_3_recs.filter(f'customer_index = {customer_index}').select('recommendations').collect()[0]\n",
    "    recommended.append([x['article_id'] for x in customer_recs])\n",
    "    recommended.append([\n",
    "        x['article_id'] for x in top_3_recs.filter(f\"customer_index = {customer_index}\"\n",
    "        ).select('recommendations').collect()[0]\\\n",
    "                   ['recommendations']\n",
    "                  ])\n",
    "    ground_truth.append([\n",
    "        x['article_id'] for x in new_test.filter(\n",
    "            new_test.customer_index==customer_index\n",
    "        ).collect()\n",
    "    ])\n",
    "\n",
    "results = spark.sparkContext.parallelize([(recommended, ground_truth)])\n",
    "baseline_model_metrics = RankingMetrics(results)\n",
    "print(baseline_model_metrics.meanAveragePrecisionAt(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30c7c933",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 14:55:16 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 14:55:39 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n",
      "23/03/29 14:55:55 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 14:56:13 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 14:56:13 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 14:56:14 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 14:56:16 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "23/03/29 14:56:24 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|customer_index|     recommendations|\n",
      "+--------------+--------------------+\n",
      "|        121486|[[879176002, 0.91...|\n",
      "|          4066|[[848175001, 1.69...|\n",
      "|         14094|[[734895002, 1.18...|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.recommendForUserSubset(test_customers, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df4ae4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 14:19:41 WARN DAGScheduler: Broadcasting large task binary with size 19.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|       id|            features|\n",
      "+---------+--------------------+\n",
      "|126589010|[0.3124433, 0.183...|\n",
      "|153115020|[-0.2535459, 0.15...|\n",
      "|153115040|[-0.25943932, 0.4...|\n",
      "+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 250:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.itemFactors.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c895b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78db1f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fractions = historical_ratings.select('article_id').distinct()\\\n",
    ".withColumn(\"fraction\", F.lit(0.7)).rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da1c4cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of \"reviews\": [Row(avg(count)=10.43766150852765)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation: [Row(stddev_samp(count)=19.892564890781237)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 225:============================================>        (169 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles with fewer than 10 \"reviews\": 23717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "article_reviews = historical_ratings.groupBy('article_id').count()\n",
    "print(f'Mean number of \"reviews\": {article_reviews.agg(F.mean(\"count\")).collect()}')\n",
    "print(f'Standard deviation: {article_reviews.agg(F.stddev(\"count\")).collect()}')\n",
    "print(f'Articles with fewer than 10 \"reviews\": {article_reviews.filter(\"count < 10\").count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2abda06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32893"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_ratings.select('article_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "851a4e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9176"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32893 - 23717"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab7ace",
   "metadata": {},
   "source": [
    "Filter for articles which have been purchased by more than 10 distinct customers.\n",
    "\n",
    "ONLY include these interactions in our train and test sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "382befdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|article_id|\n",
      "+----------+\n",
      "| 644797009|\n",
      "| 757813009|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article_reviews.filter('count < 10').select('article_id').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edc09ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|customer_id|article_id|sum(count)|\n",
      "+-----------+----------+----------+\n",
      "|       3240|         9|         2|\n",
      "|       2314|        18|         4|\n",
      "+-----------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc93c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|customer_id|article_id|sum(count)|\n",
      "+-----------+----------+----------+\n",
      "|       3240|         9|         2|\n",
      "|       2314|         9|         2|\n",
      "|       1010|         5|         5|\n",
      "|       2314|         2|         1|\n",
      "|       1010|         9|         5|\n",
      "+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.filter(\"article_id in (2, 5, 9)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b74bb49a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_reviewed_articles = article_reviews.filter('count > 10').select('article_id').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1e1aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_reviewed_articles = tuple([x[0] for x in most_reviewed_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e948e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_historical_ratings=historical_ratings.filter(f\"article_id in {most_reviewed_articles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cf4645be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fractions = final_historical_ratings.select('article_id').distinct()\\\n",
    ".withColumn(\"fraction\", F.lit(0.7)).rdd.collectAsMap()\n",
    "\n",
    "train = final_historical_ratings.sampleBy(col='article_id',\n",
    "                                          fractions=fractions,\n",
    "                                          seed=312)\n",
    "\n",
    "test = final_historical_ratings.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a22b239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8464"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('article_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2432ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8464"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_historical_ratings.select('article_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2b3a72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8425"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.select('article_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdc63028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|customer_id|article_id|count|\n",
      "+-----------+----------+-----+\n",
      "|       1010|         4|    1|\n",
      "|       1010|         5|    2|\n",
      "|       1010|        19|    2|\n",
      "|       1010|         9|    2|\n",
      "|       1010|         4|    1|\n",
      "+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fake_customers = []\n",
    "for i in range(60):\n",
    "    fake_customers.append((1010, np.random.randint(1, 20), np.random.randint(1, 3)))\n",
    "for i in range(20):\n",
    "    fake_customers.append((2314, np.random.randint(1, 20), np.random.randint(1, 3)))\n",
    "for i in range(10):\n",
    "    fake_customers.append((3240, np.random.randint(1, 20), np.random.randint(1, 3)))\n",
    "\n",
    "fake_df = spark.createDataFrame(fake_customers, schema=('customer_id', 'article_id', 'count'))\n",
    "\n",
    "fake_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e9c177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = fake_df.groupBy(['customer_id', 'article_id']).agg(F.sum('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37da2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       1010|   18|\n",
      "|       3240|    7|\n",
      "|       2314|   13|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.groupBy('customer_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "929a401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       1010|   10|\n",
      "|       3240|    5|\n",
      "|       2314|    6|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake1 = fake_df.sampleBy('customer_id', {1010: 0.5, 2314: 0.5, 3240: 0.5}, seed=2)\n",
    "fake1.groupBy('customer_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab5efc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       1010|    8|\n",
      "|       3240|    2|\n",
      "|       2314|    7|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake1_counterpart = fake_df.subtract(fake1)\n",
    "fake1_counterpart.groupBy('customer_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "403a1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|customer_id|article_id|count|\n",
      "+-----------+----------+-----+\n",
      "+-----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.filter(train.article_id.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54e2665e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|customer_id|article_id|count|\n",
      "+-----------+----------+-----+\n",
      "+-----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.filter(train.customer_id.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "661a3884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|customer_id|article_id|count|\n",
      "+-----------+----------+-----+\n",
      "+-----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.filter(train['count'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a118cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "si_customer_model = si_customer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3fb5d7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_0815572446b0, handleInvalid=error"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_customer_model.setHandleInvalid('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "63088255",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_indexed = si_customer_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6641f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "si_article_model = si_article.fit(customer_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d878a283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_43115dcdabd8, handleInvalid=error"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_article_model.setHandleInvalid('keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0c7fd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_and_article_indexed = si_article_model.transform(customer_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c2e65a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 13:24:02 WARN DAGScheduler: Broadcasting large task binary with size 13.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+-------------+\n",
      "|         customer_id|article_id|count|customer_index|article_index|\n",
      "+--------------------+----------+-----+--------------+-------------+\n",
      "|adde61206265d6e1a...| 859400006|    1|      108321.0|       3645.0|\n",
      "|577a62fedef3ba595...| 828268001|    1|       14123.0|       2523.0|\n",
      "+--------------------+----------+-----+--------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_and_article_indexed.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653be681",
   "metadata": {},
   "source": [
    "---\n",
    "work from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "49f12ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 12:59:37 WARN DAGScheduler: Broadcasting large task binary with size 12.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+\n",
      "|         customer_id|article_id|count|customer_index|\n",
      "+--------------------+----------+-----+--------------+\n",
      "|adde61206265d6e1a...| 859400006|    1|      108321.0|\n",
      "|577a62fedef3ba595...| 828268001|    1|       14123.0|\n",
      "|ca07736ee34cafb16...| 682511001|    1|       23803.0|\n",
      "+--------------------+----------+-----+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "si_customer = StringIndexer(inputCol='customer_id', outputCol='customer_index')\n",
    "customer_indexed = si_customer.fit(train).transform(train)\n",
    "\n",
    "customer_indexed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "161033c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 13:00:44 WARN DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------+-------------+\n",
      "|         customer_id|article_id|count|customer_index|article_index|\n",
      "+--------------------+----------+-----+--------------+-------------+\n",
      "|adde61206265d6e1a...| 859400006|    1|      108321.0|       3645.0|\n",
      "|577a62fedef3ba595...| 828268001|    1|       14123.0|       2523.0|\n",
      "|ca07736ee34cafb16...| 682511001|    1|       23803.0|       7486.0|\n",
      "+--------------------+----------+-----+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "si_article = StringIndexer(inputCol='article_id', outputCol='article_index')\n",
    "customer_and_article_indexed = si_article.fit(customer_indexed).transform(customer_indexed)\n",
    "customer_and_article_indexed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2bd7ce",
   "metadata": {},
   "source": [
    "---\n",
    "end of work from before^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da1629",
   "metadata": {},
   "source": [
    "Quickly check for null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2c6ad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+--------------+-------------+\n",
      "|customer_id|article_id|count|customer_index|article_index|\n",
      "+-----------+----------+-----+--------------+-------------+\n",
      "+-----------+----------+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_and_article_indexed.filter(customer_and_article_indexed.article_index.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1a9c6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+--------------+-------------+\n",
      "|customer_id|article_id|count|customer_index|article_index|\n",
      "+-----------+----------+-----+--------------+-------------+\n",
      "+-----------+----------+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_and_article_indexed.filter(customer_and_article_indexed.customer_index.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6ceab024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+--------------+-------------+\n",
      "|customer_id|article_id|count|customer_index|article_index|\n",
      "+-----------+----------+-----+--------------+-------------+\n",
      "+-----------+----------+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_and_article_indexed.filter(customer_and_article_indexed['count'].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1cb13605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer_id', 'string'),\n",
       " ('article_id', 'string'),\n",
       " ('count', 'bigint'),\n",
       " ('customer_index', 'double'),\n",
       " ('article_index', 'double')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_and_article_indexed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a9b139fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|customer_id|article_id|count|\n",
      "+-----------+----------+-----+\n",
      "|          0|         0|    0|\n",
      "+-----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = train.select([F.count(F.when(F.col(c).contains('None') | \\\n",
    "                            F.col(c).contains('NULL') | \\\n",
    "                            (F.col(c) == '' ) | \\\n",
    "                            F.col(c).isNull() | \\\n",
    "                            F.isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in train.columns])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a0341d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 13:24:56 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:24:57 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:15 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:30 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:33 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:45 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:47 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:47 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/03/29 13:25:47 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/03/29 13:25:48 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:49 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/03/29 13:25:49 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "23/03/29 13:25:49 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:51 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:52 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:53 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:54 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:55 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:56 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:58 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:25:59 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:00 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:01 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:03 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:04 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:05 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:06 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:07 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:08 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:09 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:10 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "23/03/29 13:26:11 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALSModel: uid=ALS_030f5a368ff2, rank=10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(rank=10,\n",
    "          userCol='customer_index',\n",
    "          itemCol='article_index',\n",
    "          ratingCol='count',\n",
    "          coldStartStrategy='drop',\n",
    "          seed=312\n",
    "         )\n",
    "\n",
    "model = als.fit(customer_and_article_indexed)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "54d8bf32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 12:50:14 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB\n",
      "23/03/29 12:50:16 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 5.0 in stage 291.0 (TID 11670)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 651998b7484cb33845bef058d7833826a8026eafb1ce79b6e3428b78b130b0df. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 3.0 in stage 291.0 (TID 11668)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: d7fe1eca6e71164621f938ce6f99f9bf5127de53bc9125fe8b0db744739e2d1e. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 2.0 in stage 291.0 (TID 11667)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: c01e8d3c88067d5cda04185f27bfc660f129bf6e52ea6c2f563448aecf71031d. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 7.0 in stage 291.0 (TID 11672)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 1524ba8218ba921e57dec756839baccc377c7b936be193528e75d7d2de309fbb. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 0.0 in stage 291.0 (TID 11665)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: a15186ffe4299f7f24dc692c7f65858d1a92ce5a245fa3e6403d17c6bb01fc9b. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 6.0 in stage 291.0 (TID 11671)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:16 ERROR Executor: Exception in task 4.0 in stage 291.0 (TID 11669)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 33f3b6958efcffded86e0adba570c79b982f747cd4a78a7ffd31c2ba4174d7ef. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:17 ERROR Executor: Exception in task 1.0 in stage 291.0 (TID 11666)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: cc557ea14e2d1753a49e3fc94f16154308b825f5dd1b5a775cf81a18b7ff894e. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "23/03/29 12:50:17 WARN TaskSetManager: Lost task 6.0 in stage 291.0 (TID 11671, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "\n",
      "23/03/29 12:50:17 ERROR TaskSetManager: Task 6 in stage 291.0 failed 1 times; aborting job\n",
      "23/03/29 12:50:17 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 291.0 failed 1 times, most recent failure: Lost task 6.0 in stage 291.0 (TID 11671, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:960)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:593)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 20 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 12:50:17 WARN TaskSetManager: Lost task 8.0 in stage 291.0 (TID 11673, rebeccas-mbp.lan, executor driver): TaskKilled (Stage cancelled)\n",
      "23/03/29 12:50:17 WARN TaskSetManager: Lost task 9.0 in stage 291.0 (TID 11674, rebeccas-mbp.lan, executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o536.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 291.0 failed 1 times, most recent failure: Lost task 6.0 in stage 291.0 (TID 11671, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:960)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:593)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecommendation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALS\n\u001b[1;32m      3\u001b[0m als \u001b[38;5;241m=\u001b[39m ALS(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      4\u001b[0m            userCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id_indexed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m            itemCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id_indexed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m            ratingCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m            coldStartStrategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      8\u001b[0m            seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m312\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/ml/base.py:129\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:321\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 321\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/ml/wrapper.py:318\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m:return: fitted Java model\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:131\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    133\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o536.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 291.0 failed 1 times, most recent failure: Lost task 6.0 in stage 291.0 (TID 11671, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1227)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:960)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:709)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:691)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:593)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$4037/0x00000008014b4040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: d8ca464a9faf1b8cf3c539b00cba6a99fd2f5bc7307aa4ce7ea0fe23a0110210. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(rank=10,\n",
    "           userCol='customer_id_indexed',\n",
    "           itemCol='article_id_indexed',\n",
    "           ratingCol='count',\n",
    "           coldStartStrategy='drop', \n",
    "           seed=312)\n",
    "\n",
    "model = als.fit(all_transformed)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a9eb5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALSModel"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731a8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[0.30789003, -0.2...|\n",
      "| 10|[-0.25225398, -0....|\n",
      "+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 11:09:01 WARN DAGScheduler: Broadcasting large task binary with size 15.5 MiB\n"
     ]
    }
   ],
   "source": [
    "model.itemFactors.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b50fad8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 151:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|         customer_id|article_id|count|\n",
      "+--------------------+----------+-----+\n",
      "|003ca8034fe32b9ba...| 836344002|    1|\n",
      "|006a06ee01161c997...| 827083004|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad759d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/29 11:09:18 WARN DAGScheduler: Broadcasting large task binary with size 15.5 MiB\n",
      "23/03/29 11:09:18 ERROR Executor: Exception in task 0.0 in stage 154.0 (TID 1734)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3528/0x00000008013ff040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 006a06ee01161c9972f6a05e197b55918cae265343f7d9799eb37f0ce8fb6189. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 17 more\n",
      "23/03/29 11:09:18 WARN TaskSetManager: Lost task 0.0 in stage 154.0 (TID 1734, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3528/0x00000008013ff040: (string) => double)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Unseen label: 006a06ee01161c9972f6a05e197b55918cae265343f7d9799eb37f0ce8fb6189. To handle unseen labels, set Param handleInvalid to keep.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n",
      "\t... 17 more\n",
      "\n",
      "23/03/29 11:09:18 ERROR TaskSetManager: Task 0 in stage 154.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o259.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 154.0 failed 1 times, most recent failure: Lost task 0.0 in stage 154.0 (TID 1734, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3528/0x00000008013ff040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Unseen label: 006a06ee01161c9972f6a05e197b55918cae265343f7d9799eb37f0ce8fb6189. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3528/0x00000008013ff040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 006a06ee01161c9972f6a05e197b55918cae265343f7d9799eb37f0ce8fb6189. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_transformed \u001b[38;5;241m=\u001b[39m si_model\u001b[38;5;241m.\u001b[39mtransform(test)\n\u001b[1;32m      2\u001b[0m all_test_transformed \u001b[38;5;241m=\u001b[39m si_model2\u001b[38;5;241m.\u001b[39mtransform(test_transformed)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mall_test_transformed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:440\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m:param n: Number of rows to show.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:131\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    133\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o259.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 154.0 failed 1 times, most recent failure: Lost task 0.0 in stage 154.0 (TID 1734, rebeccas-mbp.lan, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3528/0x00000008013ff040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Unseen label: 006a06ee01161c9972f6a05e197b55918cae265343f7d9799eb37f0ce8fb6189. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3528/0x00000008013ff040: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 006a06ee01161c9972f6a05e197b55918cae265343f7d9799eb37f0ce8fb6189. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:405)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:390)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "test_transformed = si_model.transform(test)\n",
    "all_test_transformed = si_model2.transform(test_transformed)\n",
    "\n",
    "all_test_transformed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a8d1896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='ALS_bc3e57749114', name='predictionCol', doc='prediction column name.')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommendForUserSubset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933eda7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae22a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket='h-and-m')\n",
    "    s3_objects = [obj['Key'] for obj in response['Contents']]\n",
    "    \n",
    "#     local_file_path = os.path.join(tempdir, s3_objects[0])\n",
    "#     s3.download_file('h-and-m', s3_objects[0], local_file_path)\n",
    "#     current_transaction = spark.read.csv(local_file_path)\n",
    "    \n",
    "    for s3_file in s3_objects:\n",
    "        local_file_path = os.path.join(tempdir, s3_file)\n",
    "        s3.download_file('h-and-m', s3_file, local_file_path)\n",
    "        new_transactions = spark.read.csv(local_file_path)\n",
    "        if s3_file == 'transactions0.csv':\n",
    "            transactions = new_transactions\n",
    "        else:\n",
    "            transactions = transactions.union(new_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea382d65",
   "metadata": {},
   "source": [
    "# H&M Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59666956",
   "metadata": {},
   "source": [
    "This notebook has turned into an amalgamation of i.) trying to upload the H&M transactions data to an S3 bucket so that I can use PySpark in DataBricks to handle the large volume of data (*top*) and ii.) my original attempt to build a proof of concept recommendation system for H&M using transactions data stored in a numpy array (*bottom*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee83c4",
   "metadata": {},
   "source": [
    "In this section, I connect to my S3 bucket for this project's data: h-and-m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d2e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0bb470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f6f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  h-and-m\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c14abf",
   "metadata": {},
   "source": [
    "Here, I tried to upload the zipped H&M data in this directory in its entirety to my S3 bucket. The process took hours and then seemed to time out without uploading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af2df86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionClosedError",
     "evalue": "Connection was closed before we received a valid response from endpoint URL: \"https://h-and-m.s3.us-east-2.amazonaws.com/h-and-m-data?uploadId=W79VT8TxeH.TkpwnCgagoJysLfuEx2XECddK8U2AqKSERieWxJjeNZFfiGgFjjHjVwiKiMQl6ry3h7f2_8tGJce9P0COCaQzXLT8_glBXNYZDkcSZYZ6FsMw6eOBf.Ps&partNumber=858\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mmaybe_status_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mmaybe_status_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionClosedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-539dd8f8b5e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/h-and-m-personalized-fashion-recommendations.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m s3.upload_file(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h-and-m'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h-and-m-data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mS3Transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         return transfer.upload_file(\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             extra_args=ExtraArgs, callback=Callback)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    277\u001b[0m             filename, bucket, key, extra_args, subscribers)\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;31m# If a client error was raised, add the backwards compatibility layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# that raises a S3UploadFailedError. These specific errors were only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Wait for 1 second for the server to send a response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 logger.debug(\"100 Continue response seen, \"\n\u001b[1;32m    168\u001b[0m                              \"now sending request body.\")\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'HTTP/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# From the RFC:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_message_body\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    201\u001b[0m                          \"Not sending data.\")\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_100_continue_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding file using iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                 \u001b[0mdatablock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mamount_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount_to_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# the stream being read from encountered any issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Wait for 1 second for the server to send a response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 logger.debug(\"100 Continue response seen, \"\n\u001b[1;32m    168\u001b[0m                              \"now sending request body.\")\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'HTTP/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# From the RFC:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_message_body\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    201\u001b[0m                          \"Not sending data.\")\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_100_continue_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding file using iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                 \u001b[0mdatablock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mamount_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount_to_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# the stream being read from encountered any issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Wait for 1 second for the server to send a response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 logger.debug(\"100 Continue response seen, \"\n\u001b[1;32m    168\u001b[0m                              \"now sending request body.\")\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'HTTP/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# From the RFC:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_message_body\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    201\u001b[0m                          \"Not sending data.\")\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_100_continue_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding file using iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                 \u001b[0mdatablock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mamount_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount_to_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# the stream being read from encountered any issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Wait for 1 second for the server to send a response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 logger.debug(\"100 Continue response seen, \"\n\u001b[1;32m    168\u001b[0m                              \"now sending request body.\")\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'HTTP/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# From the RFC:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_message_body\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    201\u001b[0m                          \"Not sending data.\")\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_100_continue_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding file using iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                 \u001b[0mdatablock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mamount_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount_to_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# the stream being read from encountered any issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mrequest_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             urllib_response = conn.urlopen(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1254\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         rval = super(AWSConnection, self)._send_request(\n\u001b[0m\u001b[1;32m     92\u001b[0m             method, url, body, headers, *args, **kwargs)\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expect_header_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# Wait for 1 second for the server to send a response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_expect_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_handle_expect_response\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 logger.debug(\"100 Continue response seen, \"\n\u001b[1;32m    168\u001b[0m                              \"now sending request body.\")\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'HTTP/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# From the RFC:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36m_send_message_body\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_message_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/awsrequest.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    201\u001b[0m                          \"Not sending data.\")\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWSConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_100_continue_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_status_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding file using iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                 \u001b[0mdatablock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mamount_to_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount_to_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callbacks_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amount)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# the stream being read from encountered any issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# main() method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_and_set_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         )\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;31m# If the task is the final task, then set the TransferFuture's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# value to the return value from main().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, client, fileobj, bucket, key, upload_id, part_number, extra_args)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \"\"\"\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             response = client.upload_part(\n\u001b[0m\u001b[1;32m    720\u001b[0m                 \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0mUploadId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupload_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartNumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpart_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             http, parsed_response = self._make_request(\n\u001b[0m\u001b[1;32m    663\u001b[0m                 operation_model, request_dict, request_context)\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    134\u001b[0m         success_response, exception = self._get_response(\n\u001b[1;32m    135\u001b[0m             request, operation_model, context)\n\u001b[0;32m--> 136\u001b[0;31m         while self._needs_retry(attempts, operation_model, request_dict,\n\u001b[0m\u001b[1;32m    137\u001b[0m                                 success_response, exception):\n\u001b[1;32m    138\u001b[0m             \u001b[0mattempts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_needs_retry\u001b[0;34m(self, attempts, operation_model, request_dict, response, caught_exception)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mservice_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             operation_model.name)\n\u001b[0;32m--> 253\u001b[0;31m         responses = self._event_emitter.emit(\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0moperation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  \u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0memit_until_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers_to_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Event %s: calling handler %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_on_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempts, response, caught_exception, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retry needed, action of: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         should_retry = self._should_retry(attempt_number, response,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                           caught_exception)\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_retry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m_should_retry\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# If we've exceeded the max attempts we just let the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# propogate if one has occurred.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchecker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             checker_response = checker(attempt_number, response,\n\u001b[0m\u001b[1;32m    317\u001b[0m                                        caught_exception)\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchecker_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcaught_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             return self._check_caught_exception(\n\u001b[0m\u001b[1;32m    223\u001b[0m                 attempt_number, caught_exception)\n\u001b[1;32m    224\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/retryhandler.py\u001b[0m in \u001b[0;36m_check_caught_exception\u001b[0;34m(self, attempt_number, caught_exception)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the MaxAttemptsDecorator is not interested in retrying the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# then this exception just propogates out past the retry code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mcaught_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_do_get_response\u001b[0;34m(self, request, operation_model)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             raise ConnectionClosedError(\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionClosedError\u001b[0m: Connection was closed before we received a valid response from endpoint URL: \"https://h-and-m.s3.us-east-2.amazonaws.com/h-and-m-data?uploadId=W79VT8TxeH.TkpwnCgagoJysLfuEx2XECddK8U2AqKSERieWxJjeNZFfiGgFjjHjVwiKiMQl6ry3h7f2_8tGJce9P0COCaQzXLT8_glBXNYZDkcSZYZ6FsMw6eOBf.Ps&partNumber=858\"."
     ]
    }
   ],
   "source": [
    "filename='data/h-and-m-personalized-fashion-recommendations.zip'\n",
    "\n",
    "s3.upload_file(\n",
    "    filename, 'h-and-m', 'h-and-m-data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5e9d9",
   "metadata": {},
   "source": [
    "As a work-around, I decided to load the transactions data into a Pandas dataframe (which takes a long-ish time, but still less than a minute) and then split it into packets which can be uploaded separately to the S3 bucket through the AWS console. This is not ideal, but it's better than being stuck for days trying to understand how the AWS connection works, data upload speeds, etc.\n",
    "\n",
    "I'm going to copy some of the code from below in my original project proof of concept here, so that I can keep all of my work for uploading the data to an S3 bucket in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27556739",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# imports\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load ALL transactions into a Pandas dataframe\n",
    "with ZipFile('data/h-and-m-personalized-fashion-recommendations.zip') as zipArchive:\n",
    "    with zipArchive.open('transactions_train.csv') as f:\n",
    "        transactions = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138e864",
   "metadata": {},
   "source": [
    "Split the data into 64 packets, roughly equally sized though it doesn't need to be exact. Save each one to a .csv file in a folder called \"packets\" within the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154f8a6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-65836da3d6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpacket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"data/packets/transactions{i}.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "indices = [int(x) for x in np.linspace(0, transactions.shape[0], 64)]\n",
    "\n",
    "for i in range(len(indices)-1):\n",
    "    start = indices[i]\n",
    "    stop = indices[i+1]\n",
    "    packet = transactions.loc[start:stop, :].copy()\n",
    "    with open(f\"data/packets/transactions{i}.csv\", \"x\") as f:\n",
    "        packet.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e990f",
   "metadata": {},
   "source": [
    "# H&M Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299100c7",
   "metadata": {},
   "source": [
    "This notebook is my proof-of-concept for a recommendation system for H&M to provide personalized recommendations to users while browsing their website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09fc59",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "[Source](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7908e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c075df65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e6121",
   "metadata": {},
   "source": [
    "Let's load the tabular data on transactions first. This will be the bread and butter of our recommendation system (though we need to address the cold start problem and look at ways of improving beyond simple matrix factorization.) The data is large, so out of curiosity, I am going to time the execution of the cell below which reads in the transaction data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf9a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 s, sys: 3.67 s, total: 35.6 s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ZipFile('data/h-and-m-personalized-fashion-recommendations.zip') as zipArchive:\n",
    "    with zipArchive.open('transactions_train.csv') as f:\n",
    "        transactions = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd440060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 5)\n"
     ]
    }
   ],
   "source": [
    "# how many records and features are in the full transactions file?\n",
    "print(transactions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39bb5ae",
   "metadata": {},
   "source": [
    "There is no need to use all 31 million + transactions to develop a proof of concept! I am going to take a random sample of 1 million transactions (which is about 3% of the data) to develop a proof of concept model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb9f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_sample = transactions.sample(1000000, random_state=seed).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30fd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file, so I don't have to reload entire transactions dataset everytime\n",
    "with open(\"data/transactions_sample.csv\", \"x\") as f:\n",
    "    transactions_sample.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4b57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that file is saved\n",
    "transactions_sample = pd.read_csv('data/transactions_sample.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed2d77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6819188</th>\n",
       "      <td>2019-02-28</td>\n",
       "      <td>cf4254f236a7d3f4d13590ab316524c245113a755786ad...</td>\n",
       "      <td>639192002</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937255</th>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>9da5d0fc26ed578538293270b501d5d040b85c43b27743...</td>\n",
       "      <td>652361003</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30574525</th>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>52d409edafe2ab3c946d7b6a9bacc1cc6f09fe31a0fa9e...</td>\n",
       "      <td>877273001</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990198</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>7022f60068ed9d8cb9270bdc4cf070d5054b9ba026a450...</td>\n",
       "      <td>714543001</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18501631</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>4d4f9a5031d13fdd485d15a0c51b611db30a1a8bd5b45e...</td>\n",
       "      <td>765853019</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               t_dat                                        customer_id  \\\n",
       "6819188   2019-02-28  cf4254f236a7d3f4d13590ab316524c245113a755786ad...   \n",
       "1937255   2018-10-30  9da5d0fc26ed578538293270b501d5d040b85c43b27743...   \n",
       "30574525  2020-08-21  52d409edafe2ab3c946d7b6a9bacc1cc6f09fe31a0fa9e...   \n",
       "6990198   2019-03-05  7022f60068ed9d8cb9270bdc4cf070d5054b9ba026a450...   \n",
       "18501631  2019-11-01  4d4f9a5031d13fdd485d15a0c51b611db30a1a8bd5b45e...   \n",
       "\n",
       "          article_id     price  sales_channel_id  \n",
       "6819188    639192002  0.067780                 1  \n",
       "1937255    652361003  0.008458                 2  \n",
       "30574525   877273001  0.033881                 1  \n",
       "6990198    714543001  0.012542                 1  \n",
       "18501631   765853019  0.067780                 2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453c701",
   "metadata": {},
   "source": [
    "Within this sample, we have:\n",
    "\n",
    "| Number of Unique Customers | Number of Unique Articles |\n",
    "| --- | --- |\n",
    "| 486,887 | 71,886 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966f0a9",
   "metadata": {},
   "source": [
    "In our entire dataset, we have:\n",
    "\n",
    "| Number of Unique Customers | Number of Unique Articles |\n",
    "| --- | --- |\n",
    "| 1,371,980 | 105,542 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b257f",
   "metadata": {},
   "source": [
    "Practically speaking, I don't think it's a problem that we are not representing anywhere close to all customers in the system in our sample.\n",
    "\n",
    "Having the unique articles represented seems more important to me. Hopefully three quarters of them is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa91fd",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd63f6",
   "metadata": {},
   "source": [
    "The code below plots the percentage of transactions accounted for by the first $n$ articles listed by popularity. I plan to use this plot to eyeball a \"head\" for my items and get a sense of how my recommender may struggle with the long tail problem for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70a15dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(range(1, len(transactions_sample.article_id.value_counts()) + 1),\n",
    "         np.cumsum(transactions_sample.article_id.value_counts(normalize=True)).values)\n",
    "ax.set(title='Percentage of Transactions Accounted for by Top n Products',\n",
    "       xlabel = 'n',\n",
    "       ylabel = 'Percentage of Transactions');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc148ed9",
   "metadata": {},
   "source": [
    "Out of a little over 70,000 unique products in our sample, more than half of transactions involve less than 10,000 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7777a94",
   "metadata": {},
   "source": [
    "Let's see if we can plot a long tail plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442ff37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of transactions including each article\n",
    "article_popularity = transactions_sample.article_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9629d3",
   "metadata": {},
   "source": [
    "The value counts series above (for the article_id column of transactions_sample) links each article id to the number of times that article was purchased.\n",
    "\n",
    "| article_id | number of purchases |\n",
    "| --- | --- |\n",
    "|  |  |\n",
    "\n",
    "Below, I reset the index of this series to reference each article by its relative popularity, rather than its unique id.\n",
    "\n",
    "For example: index 0 refers to the number 1 most popular item, index 1 refers to the second most popular item, etc.\n",
    "\n",
    "The values in the series still specify how many transactions involved that article. From this series, we can generate our long tail plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d77fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot number of transactions versus nth most popular article\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(article_popularity.reset_index().drop(['index'], axis=1))\n",
    "ax.set(title='Long Tail Plot', xlabel='nth Most Popular Article', ylabel='Transactions')\n",
    "\n",
    "# highlight the point at which less than 100 transactions include an item\n",
    "article_no = list(article_popularity.values).index(100)\n",
    "\n",
    "plt.vlines(x=article_no,\n",
    "           ymin=0,\n",
    "           ymax=max(article_popularity.values),\n",
    "           linestyles='dashed',\n",
    "           color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3fe37",
   "metadata": {},
   "source": [
    "Most of our transactions involved articles which were purchased fewer than 100 times. This could be a good thing for training our recommendation system, if most of the items are in the long tail. However, we may want to drop the 1074 most popular items (purchased more than 100 times each) if the system is struggling to provide coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ac491",
   "metadata": {},
   "source": [
    "Out of curiosity and practicality, how many items were only purchased once?\n",
    "\n",
    "---\n",
    "\n",
    "14,821 items were only purchased once.\n",
    "\n",
    "1,074 items have been purchased over 100 times.\n",
    "\n",
    "55,991 items are \"in the middle\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58b6f4",
   "metadata": {},
   "source": [
    "This is my first time building a recommendation system, so I want to take a moment to think through how I should structure this project.\n",
    "\n",
    "- explore metadata on articles & customers\n",
    "- decide on an evaluation metric\n",
    "- determine a validation strategy\n",
    "- decide how to pull the data together: use article_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36d4a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 41.5 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ZipFile('data/h-and-m-personalized-fashion-recommendations.zip') as zipArchive:\n",
    "    with zipArchive.open('articles.csv') as f:\n",
    "        articles = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dde01c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>product_code</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_no</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_no</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_code</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>...</th>\n",
       "      <th>index_code</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_no</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_no</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_no</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>9</td>\n",
       "      <td>Black</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>108775</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>253</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>1010016</td>\n",
       "      <td>Solid</td>\n",
       "      <td>10</td>\n",
       "      <td>White</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>16</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>1002</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  product_code  prod_name  product_type_no product_type_name  \\\n",
       "0   108775015        108775  Strap top              253          Vest top   \n",
       "1   108775044        108775  Strap top              253          Vest top   \n",
       "\n",
       "   product_group_name  graphical_appearance_no graphical_appearance_name  \\\n",
       "0  Garment Upper body                  1010016                     Solid   \n",
       "1  Garment Upper body                  1010016                     Solid   \n",
       "\n",
       "   colour_group_code colour_group_name  ...  index_code  index_name  \\\n",
       "0                  9             Black  ...           A  Ladieswear   \n",
       "1                 10             White  ...           A  Ladieswear   \n",
       "\n",
       "   index_group_no index_group_name  section_no            section_name  \\\n",
       "0               1       Ladieswear          16  Womens Everyday Basics   \n",
       "1               1       Ladieswear          16  Womens Everyday Basics   \n",
       "\n",
       "  garment_group_no garment_group_name  \\\n",
       "0             1002       Jersey Basic   \n",
       "1             1002       Jersey Basic   \n",
       "\n",
       "                               detail_desc image  \n",
       "0  Jersey top with narrow shoulder straps.  None  \n",
       "1  Jersey top with narrow shoulder straps.  None  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd9edf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105542, 26)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05851cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 231 ms, total: 4.28 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ZipFile('data/h-and-m-personalized-fashion-recommendations.zip') as zipArchive:\n",
    "    with zipArchive.open('customers.csv') as f:\n",
    "        customers = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f44bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>FN</th>\n",
       "      <th>Active</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>fashion_news_frequency</th>\n",
       "      <th>age</th>\n",
       "      <th>postal_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52043ee2162cf5aa7ee79974281641c6f11a68d276429a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2973abc54daa8a5f8ccfe9362140c63247c5eee03f1d93...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  FN  Active  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3... NaN     NaN   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e... NaN     NaN   \n",
       "\n",
       "  club_member_status fashion_news_frequency   age  \\\n",
       "0             ACTIVE                   NONE  49.0   \n",
       "1             ACTIVE                   NONE  25.0   \n",
       "\n",
       "                                         postal_code  \n",
       "0  52043ee2162cf5aa7ee79974281641c6f11a68d276429a...  \n",
       "1  2973abc54daa8a5f8ccfe9362140c63247c5eee03f1d93...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "991a88ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1371980, 7)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648fcfb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff00d3",
   "metadata": {},
   "source": [
    "I am going to select 800,000 transactions to constitute the training data and 200,000 transactions to make up the hold out test set.\n",
    "\n",
    "Then, I am going to further subdivide the training data into true training and validation data with a 75% / 25% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab32d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data (including validation)\n",
    "train = transactions_sample.sample(800000, random_state=seed).copy()\n",
    "# which indices are \"leftover\" after selecting training data?\n",
    "test_indices = list(set(transactions_sample.index) - set(train.index))\n",
    "# get test data\n",
    "test = transactions_sample.loc[ test_indices ].copy()\n",
    "\n",
    "# get validation data\n",
    "val = train.sample(200000, random_state=seed).copy()\n",
    "train_indices = list(set(train.index) - set(val.index))\n",
    "\n",
    "# reset train to NOT include validation data\n",
    "train = train.loc[ train_indices ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0625c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 5)\n",
      "(200000, 5)\n",
      "(200000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98223f96",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "Building a recommendation system which uses alternating least squares to obtain a PQ factorization of the utility matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9904d06",
   "metadata": {},
   "source": [
    "### Build Utility Matrix from Customers & Articles in Training Transaction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05570523",
   "metadata": {},
   "source": [
    "Recall that the utility matrix refers to the user-item ratings matrix.\n",
    "\n",
    "In our case, we are using implicit ratings = number of times that a customer has bought an article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e2a85",
   "metadata": {},
   "source": [
    "Get the unique customers and articles in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f77252",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_customers = np.unique(train.customer_id)\n",
    "unique_articles = np.unique(train.article_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5fb9c",
   "metadata": {},
   "source": [
    "Label encode the unique customers and articles so that you can index the utility matrix in base numpy. (Only Pandas lets the index and column names be non-integer or non-consecutive integer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1899fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "customer_le = LabelEncoder()\n",
    "article_le = LabelEncoder()\n",
    "\n",
    "customer_code = customer_le.fit_transform(unique_customers)\n",
    "article_code = article_le.fit_transform(unique_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e9ae3",
   "metadata": {},
   "source": [
    "Create a matrix of zeros of the appropriate size for the utility matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fa9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility = np.zeros((len(customer_code), len(article_code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b070459",
   "metadata": {},
   "source": [
    "Grab transaction information from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d18c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions = train.loc[:, ['customer_id', 'article_id']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dcab44",
   "metadata": {},
   "source": [
    "Label encode the customer and article ids in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f838c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions['customer_id'] = customer_le.transform(train_transactions['customer_id'])\n",
    "train_transactions['article_id'] = article_le.transform(train_transactions['article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25580ea1",
   "metadata": {},
   "source": [
    "Iterate over each row representing a transaction in the training data, and add 1 to the appropriate \\[user, item\\] entry in the utility matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f9ed113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(train_transactions.index):\n",
    "    code = train_transactions.loc[index, 'customer_id']\n",
    "    article = train_transactions.loc[index, 'article_id']\n",
    "    \n",
    "    utility[code, article] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb51415",
   "metadata": {},
   "source": [
    "*To save space, checking the utility matrix has been deleted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73346e6e",
   "metadata": {},
   "source": [
    "### Use Alternating Least Squares to Approximate $PQ^T$ Factorization of Utility Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a182a",
   "metadata": {},
   "source": [
    "To get a handle on the process, let's start with $k=5$ latent factors, and build the rank 5 $PQ^{T}$ approximation of the utility / ratings matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bfded",
   "metadata": {},
   "source": [
    "The idea behind alternating least squares is to \"guess\" the values of P and Q, then iteratively update these entries according to least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2023b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "users = np.ones((utility.shape[0], k))\n",
    "items = np.ones((utility.shape[1], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dad7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "for row in range(utility.shape[0]):\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    # rows of items are \"records\", target is first row of utility matrix\n",
    "    lr.fit(items, utility[row,:].reshape(-1, 1))\n",
    "\n",
    "    users[row, :] = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(utility.shape[1]):\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    \n",
    "    lr.fit(users, utility[:, col])\n",
    "    \n",
    "    items[:, col] = lr.coef_[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
